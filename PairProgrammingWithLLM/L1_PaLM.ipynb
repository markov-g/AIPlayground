{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308de0a-2fb6-47db-a9e0-508cdb3213fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "def get_api_key():    \n",
    "    _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "    return os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb5e78-1221-4073-ad52-cc1e15292044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as palm\n",
    "from google.api_core import client_options as client_options_lib\n",
    "\n",
    "palm.configure(\n",
    "    api_key=get_api_key(),\n",
    "    transport=\"rest\",\n",
    "    client_options=client_options_lib.ClientOptions(\n",
    "        api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce5d4e5-5c71-463c-83e6-4db01c0190fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in palm.list_models():\n",
    "    print(f\"name: {m.name}\")\n",
    "    print(f\"description: {m.description}\")\n",
    "    print(f\"generation methods:{m.supported_generation_methods}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d7d0d-fd4e-4518-b063-21403db8eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75e22ee-d9c4-4539-b697-71059d770c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bison = models[0]\n",
    "model_bison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48ffd1-4be6-444c-aac2-6934b4836db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core import retry\n",
    "@retry.Retry()\n",
    "def generate_text(prompt,\n",
    "                  model=model_bison,\n",
    "                  temperature=0.0):\n",
    "    return palm.generate_text(prompt=prompt,\n",
    "                              model=model,\n",
    "                              temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99605ccb-8a28-433d-9a6e-e5dca743c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Show me how to iterate across a list in Python.\"\n",
    "completion = generate_text(prompt)\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fe1340b718b7ce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Add Templeting to prime PaLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd7cb03756ce788",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Prompt template\n",
    "\n",
    "1. priming: getting the LLM ready for the type of task you'll ask it to do.\n",
    "2. question: the specific task.\n",
    "3. decorator: how to provide or format the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50320064926d2cc0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "{priming}\n",
    "\n",
    "{question}\n",
    "\n",
    "{decorator}\n",
    "\n",
    "Your solution:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1372e96c20fce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "priming_text =  \"\"\"\n",
    "You are an expert in writing clear, concise, Python code. \n",
    "\"\"\"\n",
    "\n",
    "question = \"\"\"\n",
    "Create a doubly linked list.\n",
    "\"\"\"\n",
    "\n",
    "decorator = \"\"\"\n",
    "Insert comments for each line of code. Make sure you make the solution understandeable for a newbie.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b83e564d2d429",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(\n",
    "    priming=priming_text,\n",
    "    question=question,\n",
    "    decorator=decorator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265d64c5d9a35d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(prompt)\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388280bf9701a80",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "create a very large list of random numbers in python, and then write code to sort that list\n",
    "\"\"\"\n",
    "prompt = prompt_template.format(priming=priming_text,\n",
    "                                question=question,\n",
    "                                decorator=decorator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262bfb3f14dd0599",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(prompt)\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe4bdf5d88e04f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scenario 1: Improve existing code\n",
    "- An LLM can help you rewrite your code in the way that's recommended for that particular language.\n",
    "- You can ask an LLM to rewrite your Python code in a way that is more 'Pythonic\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9b802c3934619",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "I don't think this code is the best way to do it in Python, can you help me?\n",
    "\n",
    "{question}\n",
    "\n",
    "Please explain, in detail, what you did to improve it.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0212a65aa32e03",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "def func_x(array)\n",
    "  for i in range(len(array)):\n",
    "    print(array[i])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dbce7235cb3bb8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065637844c4e722",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Ask for multiple ways of rewriting your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6fc12bf96511",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "I don't think this code is the best way to do it in Python, can you help me?\n",
    "\n",
    "{question}\n",
    "\n",
    "Please explore multiple ways of solving the problem, and explain each.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688e9ffce6b1f12",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1948cfec8931f9f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scenario 2: Simplify code\n",
    "- Ask the LLM to perform a code review.\n",
    "- Note that adding/removing newline characters may affect the LLM completion that gets output by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae2797d789ba6e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# option 2\n",
    "prompt_template = \"\"\"\n",
    "Can you please simplify this code for a linked list in Python? \\n\n",
    "You are an expert in Pythonic code.\n",
    "\n",
    "{question}\n",
    "\n",
    "Please comment each line in detail, \\n\n",
    "and explain in detail what you did to modify it, and why.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce69d4af4eaf40",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "class Node:\n",
    "  def __init__(self, dataval=None):\n",
    "    self.dataval = dataval\n",
    "    self.nextval = None\n",
    "\n",
    "class SLinkedList:\n",
    "  def __init__(self):\n",
    "    self.headval = None\n",
    "\n",
    "list1 = SLinkedList()\n",
    "list1.headval = Node(\"Mon\")\n",
    "e2 = Node(\"Tue\")\n",
    "e3 = Node(\"Wed\")\n",
    "list1.headval.nextval = e2\n",
    "e2.nextval = e3\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7a7504887a80b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef4e70114cadf1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scenario 3: Write test cases\n",
    "\n",
    "- It may help to specify that you want the LLM to output \"in code\" to encourage it to write unit tests instead of just returning test cases in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ebe820762d1e9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Can you please create test cases in code for this Python code?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what these test cases are designed to achieve.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9dd332a9ce18c4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that the code I'm using here was output in the previous\n",
    "# section. Your output code may be different.\n",
    "question = \"\"\"\n",
    "class Node:\n",
    "  def __init__(self, dataval=None):\n",
    "    self.dataval = dataval\n",
    "    self.nextval = None\n",
    "\n",
    "class SLinkedList:\n",
    "  def __init__(self):\n",
    "    self.head = None\n",
    "\n",
    "def create_linked_list(data):\n",
    "  head = Node(data[0])\n",
    "  for i in range(1, len(data)):\n",
    "    node = Node(data[i])\n",
    "    node.nextval = head\n",
    "    head = node\n",
    "  return head\n",
    "\n",
    "list1 = create_linked_list([\"Mon\", \"Tue\", \"Wed\"])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e43a00e7cf5dcc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d9230d5c4fdbb8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scenario 4: Make code more efficient\n",
    "- Improve runtime by potentially avoiding inefficient methods (such as ones that use recursion when not needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f37e1c6d78dfe5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Can you please make this code more efficient?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what you changed and why.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372dd5432d147be2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "# Returns index of x in arr if present, else -1\n",
    "def binary_search(arr, low, high, x):\n",
    "    # Check base case\n",
    "    if high >= low:\n",
    "        mid = (high + low) // 2\n",
    "        if arr[mid] == x:\n",
    "            return mid\n",
    "        elif arr[mid] > x:\n",
    "            return binary_search(arr, low, mid - 1, x)\n",
    "        else:\n",
    "            return binary_search(arr, mid + 1, high, x)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Test array\n",
    "arr = [ 2, 3, 4, 10, 40 ]\n",
    "x = 10\n",
    "\n",
    "# Function call\n",
    "result = binary_search(arr, 0, len(arr)-1, x)\n",
    "\n",
    "if result != -1:\n",
    "    print(\"Element is present at index\", str(result))\n",
    "else:\n",
    "    print(\"Element is not present in array\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b072589b88db64",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb959bd8e65f58",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scenario 5: Debug your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd41f78a614e40",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Can you please help me to debug this code?\n",
    "\n",
    "{question}\n",
    "\n",
    "Explain in detail what you found and why it was a bug.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e08f136cf3c2cc8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I deliberately introduced a bug into this code! Let's see if the LLM can find it.\n",
    "# Note -- the model can't see this comment -- but the bug is in the\n",
    "# print function. There's a circumstance where nodes can be null, and trying\n",
    "# to print them would give a null error.\n",
    "question = \"\"\"\n",
    "class Node:\n",
    "   def __init__(self, data):\n",
    "      self.data = data\n",
    "      self.next = None\n",
    "      self.prev = None\n",
    "\n",
    "class doubly_linked_list:\n",
    "   def __init__(self):\n",
    "      self.head = None\n",
    "\n",
    "# Adding data elements\n",
    "   def push(self, NewVal):\n",
    "      NewNode = Node(NewVal)\n",
    "      NewNode.next = self.head\n",
    "      if self.head is not None:\n",
    "         self.head.prev = NewNode\n",
    "      self.head = NewNode\n",
    "\n",
    "# Print the Doubly Linked list in order\n",
    "   def listprint(self, node):\n",
    "       print(node.data),\n",
    "       last = node\n",
    "       node = node.next\n",
    "\n",
    "dllist = doubly_linked_list()\n",
    "dllist.push(12)\n",
    "dllist.push(8)\n",
    "dllist.push(62)\n",
    "dllist.listprint(dllist.head)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833337a0487cc10d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice in this case that we are using the default temperature of `0.7` to generate the example that you're seeing in the lecture video.  \n",
    "- Since a temperature > 0 encourages more randomness in the LLM output, you may want to run this code a couple times to see what it outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504e2fc7158c89f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=question),\n",
    "    temperature = 0.7\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab59671d95edc9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scenario 6: Ask an LLM to explain a complex code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113d678bac73024",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@title Complex Code Block\n",
    "# Note: Taken from https://github.com/lmoroney/odmlbook/blob/63c0825094b2f44efc5c4d3226425a51990e73d6/BookSource/Chapter08/ios/cats_vs_dogs/CatVsDogClassifierSample/ModelDataHandler/ModelDataHandler.swift\n",
    "CODE_BLOCK = \"\"\"\n",
    "// Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n",
    "//\n",
    "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "// you may not use this file except in compliance with the License.\n",
    "// You may obtain a copy of the License at\n",
    "//\n",
    "//    http://www.apache.org/licenses/LICENSE-2.0\n",
    "//\n",
    "// Unless required by applicable law or agreed to in writing, software\n",
    "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "// See the License for the specific language governing permissions and\n",
    "// limitations under the License.\n",
    "\n",
    "import CoreImage\n",
    "import TensorFlowLite\n",
    "import UIKit\n",
    "\n",
    "\n",
    "/// An inference from invoking the `Interpreter`.\n",
    "struct Inference {\n",
    "  let confidence: Float\n",
    "  let label: String\n",
    "}\n",
    "\n",
    "/// Information about a model file or labels file.\n",
    "typealias FileInfo = (name: String, extension: String)\n",
    "\n",
    "/// Information about the MobileNet model.\n",
    "enum MobileNet {\n",
    "  static let modelInfo: FileInfo = (name: \"converted_model\", extension: \"tflite\")\n",
    "}\n",
    "\n",
    "/// This class handles all data preprocessing and makes calls to run inference on a given frame\n",
    "/// by invoking the `Interpreter`. It then formats the inferences obtained and returns the top N\n",
    "/// results for a successful inference.\n",
    "class ModelDataHandler {\n",
    "\n",
    "  // MARK: - Public Properties\n",
    "\n",
    "  /// The current thread count used by the TensorFlow Lite Interpreter.\n",
    "  let threadCount: Int\n",
    "\n",
    "  let resultCount = 1\n",
    "\n",
    "  // MARK: - Model Parameters\n",
    "\n",
    "  let batchSize = 1\n",
    "  let inputChannels = 3\n",
    "  let inputWidth = 224\n",
    "  let inputHeight = 224\n",
    "\n",
    "  // MARK: - Private Properties\n",
    "\n",
    "  /// List of labels from the given labels file.\n",
    "  private var labels: [String] = [\"Cat\", \"Dog\"]\n",
    "\n",
    "  /// TensorFlow Lite `Interpreter` object for performing inference on a given model.\n",
    "  private var interpreter: Interpreter\n",
    "\n",
    "  /// Information about the alpha component in RGBA data.\n",
    "  private let alphaComponent = (baseOffset: 4, moduloRemainder: 3)\n",
    "\n",
    "  // MARK: - Initialization\n",
    "\n",
    "  /// A failable initializer for `ModelDataHandler`. A new instance is created if the model and\n",
    "  /// labels files are successfully loaded from the app's main bundle. Default `threadCount` is 1.\n",
    "  init?(modelFileInfo: FileInfo, threadCount: Int = 1) {\n",
    "    let modelFilename = modelFileInfo.name\n",
    "\n",
    "    // Construct the path to the model file.\n",
    "    guard let modelPath = Bundle.main.path(\n",
    "      forResource: modelFilename,\n",
    "      ofType: modelFileInfo.extension\n",
    "      ) else {\n",
    "        print(\"Failed to load the model file with name: \\(modelFilename).\")\n",
    "        return nil\n",
    "    }\n",
    "\n",
    "    // Specify the options for the `Interpreter`.\n",
    "    self.threadCount = threadCount\n",
    "    var options = InterpreterOptions()\n",
    "    options.threadCount = threadCount\n",
    "    do {\n",
    "      // Create the `Interpreter`.\n",
    "      interpreter = try Interpreter(modelPath: modelPath, options: options)\n",
    "    } catch let error {\n",
    "      print(\"Failed to create the interpreter with error: \\(error.localizedDescription)\")\n",
    "      return nil\n",
    "    }\n",
    "\n",
    "  }\n",
    "\n",
    "  // MARK: - Public Methods\n",
    "\n",
    "  /// Performs image preprocessing, invokes the `Interpreter`, and process the inference results.\n",
    "  func runModel(onFrame pixelBuffer: CVPixelBuffer) -> [Inference]? {\n",
    "    let sourcePixelFormat = CVPixelBufferGetPixelFormatType(pixelBuffer)\n",
    "    assert(sourcePixelFormat == kCVPixelFormatType_32ARGB ||\n",
    "      sourcePixelFormat == kCVPixelFormatType_32BGRA ||\n",
    "      sourcePixelFormat == kCVPixelFormatType_32RGBA)\n",
    "\n",
    "\n",
    "    let imageChannels = 4\n",
    "    assert(imageChannels >= inputChannels)\n",
    "\n",
    "    // Crops the image to the biggest square in the center and scales it down to model dimensions.\n",
    "    let scaledSize = CGSize(width: inputWidth, height: inputHeight)\n",
    "    guard let thumbnailPixelBuffer = pixelBuffer.centerThumbnail(ofSize: scaledSize) else {\n",
    "      return nil\n",
    "    }\n",
    "\n",
    "    let outputTensor: Tensor\n",
    "    do {\n",
    "      // Allocate memory for the model's input `Tensor`s.\n",
    "      try interpreter.allocateTensors()\n",
    "\n",
    "      // Remove the alpha component from the image buffer to get the RGB data.\n",
    "      guard let rgbData = rgbDataFromBuffer(\n",
    "        thumbnailPixelBuffer,\n",
    "        byteCount: batchSize * inputWidth * inputHeight * inputChannels\n",
    "        ) else {\n",
    "          print(\"Failed to convert the image buffer to RGB data.\")\n",
    "          return nil\n",
    "      }\n",
    "\n",
    "      // Copy the RGB data to the input `Tensor`.\n",
    "      try interpreter.copy(rgbData, toInputAt: 0)\n",
    "\n",
    "      // Run inference by invoking the `Interpreter`.\n",
    "      try interpreter.invoke()\n",
    "\n",
    "      // Get the output `Tensor` to process the inference results.\n",
    "      outputTensor = try interpreter.output(at: 0)\n",
    "    } catch let error {\n",
    "      print(\"Failed to invoke the interpreter with error: \\(error.localizedDescription)\")\n",
    "      return nil\n",
    "    }\n",
    "\n",
    "    let results = [Float32](unsafeData: outputTensor.data) ?? []\n",
    "\n",
    "    // Process the results.\n",
    "    let topNInferences = getTopN(results: results)\n",
    "\n",
    "    // Return the inference time and inference results.\n",
    "    return topNInferences\n",
    "  }\n",
    "\n",
    "  // MARK: - Private Methods\n",
    "\n",
    "  /// Returns the top N inference results sorted in descending order.\n",
    "  private func getTopN(results: [Float]) -> [Inference] {\n",
    "    // Create a zipped array of tuples [(labelIndex: Int, confidence: Float)].\n",
    "    let zippedResults = zip(labels.indices, results)\n",
    "\n",
    "    // Sort the zipped results by confidence value in descending order.\n",
    "    let sortedResults = zippedResults.sorted { $0.1 > $1.1 }.prefix(resultCount)\n",
    "\n",
    "    // Return the `Inference` results.\n",
    "    return sortedResults.map { result in Inference(confidence: result.1, label: labels[result.0]) }\n",
    "  }\n",
    "\n",
    "  /// Loads the labels from the labels file and stores them in the `labels` property.\n",
    "  private func loadLabels(fileInfo: FileInfo) {\n",
    "    let filename = fileInfo.name\n",
    "    let fileExtension = fileInfo.extension\n",
    "    guard let fileURL = Bundle.main.url(forResource: filename, withExtension: fileExtension) else {\n",
    "      fatalError(\"Labels file not found in bundle. Please add a labels file with name \" +\n",
    "        \"\\(filename).\\(fileExtension) and try again.\")\n",
    "    }\n",
    "    do {\n",
    "      let contents = try String(contentsOf: fileURL, encoding: .utf8)\n",
    "      labels = contents.components(separatedBy: .newlines)\n",
    "    } catch {\n",
    "      fatalError(\"Labels file named \\(filename).\\(fileExtension) cannot be read. Please add a \" +\n",
    "        \"valid labels file and try again.\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /// Returns the RGB data representation of the given image buffer with the specified `byteCount`.\n",
    "  ///\n",
    "  /// - Parameters\n",
    "  ///   - buffer: The pixel buffer to convert to RGB data.\n",
    "  ///   - byteCount: The expected byte count for the RGB data calculated using the values that the\n",
    "  ///       model was trained on: `batchSize * imageWidth * imageHeight * componentsCount`.\n",
    "  ///   - isModelQuantized: Whether the model is quantized (i.e. fixed point values rather than\n",
    "  ///       floating point values).\n",
    "  /// - Returns: The RGB data representation of the image buffer or `nil` if the buffer could not be\n",
    "  ///     converted.\n",
    "  private func rgbDataFromBuffer(\n",
    "    _ buffer: CVPixelBuffer,\n",
    "    byteCount: Int\n",
    "    ) -> Data? {\n",
    "    CVPixelBufferLockBaseAddress(buffer, .readOnly)\n",
    "    defer { CVPixelBufferUnlockBaseAddress(buffer, .readOnly) }\n",
    "    guard let mutableRawPointer = CVPixelBufferGetBaseAddress(buffer) else {\n",
    "      return nil\n",
    "    }\n",
    "    let count = CVPixelBufferGetDataSize(buffer)\n",
    "    let bufferData = Data(bytesNoCopy: mutableRawPointer, count: count, deallocator: .none)\n",
    "    var rgbBytes = [Float](repeating: 0, count: byteCount)\n",
    "    var index = 0\n",
    "    for component in bufferData.enumerated() {\n",
    "      let offset = component.offset\n",
    "      let isAlphaComponent = (offset % alphaComponent.baseOffset) == alphaComponent.moduloRemainder\n",
    "      guard !isAlphaComponent else { continue }\n",
    "      rgbBytes[index] = Float(component.element) / 255.0\n",
    "      index += 1\n",
    "    }\n",
    "\n",
    "    return rgbBytes.withUnsafeBufferPointer(Data.init)\n",
    "\n",
    "  }\n",
    "}\n",
    "\n",
    "// MARK: - Extensions\n",
    "\n",
    "extension Data {\n",
    "  /// Creates a new buffer by copying the buffer pointer of the given array.\n",
    "  ///\n",
    "  /// - Warning: The given array's element type `T` must be trivial in that it can be copied bit\n",
    "  ///     for bit with no indirection or reference-counting operations; otherwise, reinterpreting\n",
    "  ///     data from the resulting buffer has undefined behavior.\n",
    "  /// - Parameter array: An array with elements of type `T`.\n",
    "  init<T>(copyingBufferOf array: [T]) {\n",
    "    self = array.withUnsafeBufferPointer(Data.init)\n",
    "  }\n",
    "}\n",
    "\n",
    "extension Array {\n",
    "  /// Creates a new array from the bytes of the given unsafe data.\n",
    "  ///\n",
    "  /// - Warning: The array's `Element` type must be trivial in that it can be copied bit for bit\n",
    "  ///     with no indirection or reference-counting operations; otherwise, copying the raw bytes in\n",
    "  ///     the `unsafeData`'s buffer to a new array returns an unsafe copy.\n",
    "  /// - Note: Returns `nil` if `unsafeData.count` is not a multiple of\n",
    "  ///     `MemoryLayout<Element>.stride`.\n",
    "  /// - Parameter unsafeData: The data containing the bytes to turn into an array.\n",
    "  init?(unsafeData: Data) {\n",
    "\n",
    "    guard unsafeData.count % MemoryLayout<Element>.stride == 0 else { return nil }\n",
    "    #if swift(>=5.0)\n",
    "    self = unsafeData.withUnsafeBytes { .init($0.bindMemory(to: Element.self)) }\n",
    "    #else\n",
    "    self = unsafeData.withUnsafeBytes {\n",
    "      .init(UnsafeBufferPointer<Element>(\n",
    "        start: $0,\n",
    "        count: unsafeData.count / MemoryLayout<Element>.stride\n",
    "      ))\n",
    "    }\n",
    "    #endif  // swift(>=5.0)\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca487f84cd4668",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Can you please explain how this code works?\n",
    "\n",
    "{question}\n",
    "\n",
    "Use a lot of detail and make it as clear as possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ede34e95d67124",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48cd3bd5715dd8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scenario 7: Ask an LLM to document a complex code base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107db17702108f9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Please write technical documentation for this code and \\n\n",
    "make it easy for a non swift developer to understand:\n",
    "\n",
    "{question}\n",
    "\n",
    "Output the results in markdown\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc22369b720b2e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "completion = generate_text(\n",
    "    prompt = prompt_template.format(question=CODE_BLOCK)\n",
    ")\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a8a64b27ec44d9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
