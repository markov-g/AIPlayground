{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be4274da",
   "metadata": {},
   "source": [
    "# Getting Started with RAG in DSPy\n",
    "\n",
    "This notebook will show you how to use DSPy to compile a RAG program! DSPy compilation is a fairly new tool for LLM developers, so let's start with an overview of the concept. By `compiling`, we mean finding the prompts that elicit the behavior we want from LLMs when connected in some kind of pipeline.\n",
    "\n",
    "For example, RAG is a very common LLM pipeline. In it's simplest form, RAG consists of 2 steps, (1) Retrieve and (2) Answer a Question. Part (2), Answering a Question, has an associated prompt, for example, people generally use:\n",
    "\n",
    "```\n",
    "--\n",
    "\n",
    "Please answer the question based on the following context.\n",
    "\n",
    "context  {context}\n",
    "\n",
    "question {question}\n",
    "\n",
    "--\n",
    "```\n",
    "\n",
    "This prompt may be a good initial point for an LLM to understand the task. However, it is not the *optimal* prompt. DSPy optimizes the prompt for you by jointly (1) tweaking the instructions, such as rewriting an initial prompt like: \n",
    "\n",
    "```\n",
    "Please answer the question based on the following context.\n",
    "```\n",
    "\n",
    "to \n",
    "\n",
    "```\n",
    "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
    "```\n",
    "\n",
    "Further, DSPy (2) finds examples of desired input-outputs in the prompt to further improve performance, also known as `In-Context Learning`. In this example, we will begin with the simple prompt: `Please answer the question based on the following context.` and end up with:\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "In order to leverage black-box optimization techniques like random search, bayesian optimization, or evolutionary algorithms, we need a metric. Coming up with metrics to describe desired system behavior has been a longstanding challenge in Machine Learning research. Excitingly, LLMs have made amazing progress. For example, we can evaluate a RAG answer by prompting an LLM with, `Is the assessed text grounded in the context? Say no if it includes significant facts not in the context`. We then optimize the RAG program to increase the metric LLM's assessment of answer quality.\n",
    "\n",
    "This example contains 4 parts:\n",
    "\n",
    "- 0: DSPy Settings and Installation\n",
    "- 1: DSPy Datasets with `dspy.Example`\n",
    "- 2: LLM Metrics in DSPy\n",
    "- 3: LLM Programming with `dspy.Module`\n",
    "- 4: Optimization with `BootstrapFewShot`, `BootstrapFewShotRandomSearch`, and `BayesianSignatureOptimizer`.\n",
    "\n",
    "\n",
    "We are using 2 datasets for this example. Firstly, we have an index of the Weaviate Blog Posts. We will use the Weaviate Blog Posts as the retrieved context to help with our second dataset, the Weaviate FAQs. The Weaviate FAQs consists of 44 question-answer pairs of frequently asked Weaviate questions such as: `Do I need to know about Docker (Compose) to use Weaviate?`\n",
    "\n",
    "We isolate 10 examples to use as our test set and optimize our program with the remaining 34.\n",
    "\n",
    "Our uncompiled RAG program achieves a score of 270 on the held-out test set.\n",
    "\n",
    "Our RAG program compiled with the `BayesianSignatureOptimizer` achieves a score of 340! A ~30% improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb763b0b",
   "metadata": {},
   "source": [
    "# 0: DSPy Settings and Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a06dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install dspy-ai==2.1.9 weaviate-client==3.26.2 > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e294d08e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T03:02:28.752960Z",
     "start_time": "2024-03-18T03:02:27.872487Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai, os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5314eeb0314616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T03:02:32.555420Z",
     "start_time": "2024-03-18T03:02:32.511580Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42260862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T03:02:36.073614Z",
     "start_time": "2024-03-18T03:02:33.742139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Weaviate Retriever and configure LLM\n",
    "import dspy\n",
    "from dspy.retrieve.chromadb_rm import ChromadbRM\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().absolute()\n",
    "\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "ollamaLLM = dspy.OpenAI(api_base=\"http://localhost:11434/v1/\", \n",
    "                        api_key=\"ollama\", \n",
    "                        model=\"dolphin-mistral:latest\", stop='\\n\\n', model_type='chat')\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "# weaviate_client = weaviate.Client(\"http://localhost:8080\")\n",
    "retriever_model = ChromadbRM('BlogChunk', f'{PROJECT_ROOT}/_chroma_cache', openai_ef)\n",
    "# retriever_model = WeaviateRM(\"WeaviateBlogChunk\", weaviate_client=weaviate_client)\n",
    "# Assumes the Weaviate collection has a text key `content`\n",
    "dspy.settings.configure(lm=ollamaLLM, rm=retriever_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba0a2ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T03:02:39.777327Z",
     "start_time": "2024-03-18T03:02:39.705488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In the vast sea of data,\\nNeural networks weave their magic,\\nLearning patterns, predicting future, forever adapting.']\n",
      "['\\nIn the realm of data, where numbers dance,\\nNeural networks weave their enchanting chance,\\nLearning, adapting, in digital trance.']\n"
     ]
    }
   ],
   "source": [
    "print(dspy.settings.lm(\"Write a 3 line poem about neural networks.\"))\n",
    "context_example = dspy.OpenAI(api_base=\"http://localhost:11434/v1/\",\n",
    "                              api_key=\"ollama\",\n",
    "                              model=\"wizardlm:13b-llama2-q6_K\", stop='\\n\\n', model_type='chat')\n",
    "context_example_2 = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "with dspy.context(llm=context_example_2):\n",
    "    print(context_example(\"Write a 3 line poem about neural networks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b146a20",
   "metadata": {},
   "source": [
    "# 1. DSPy Datasets with `dspy.Example`\n",
    "\n",
    "Our retrieval engine is filled with chunks from Weaviate Blog posts.\n",
    "\n",
    "Please see weaviate/recipes/integrations/dspy/Weaviate-Import.ipynb for a full tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08417e60",
   "metadata": {},
   "source": [
    "# Import FAQs from a markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cacaa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T03:03:17.139659Z",
     "start_time": "2024-03-18T03:03:17.110216Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why would I use Weaviate as my vector database?',\n",
       " 'What is the difference between Weaviate and for example Elasticsearch?',\n",
       " 'Do you offer Weaviate as a managed service?',\n",
       " 'How should I configure the size of my instance?',\n",
       " 'Do I need to know about Docker (Compose) to use Weaviate?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load FAQs\n",
    "import re\n",
    "\n",
    "# f = open(\"faq.md\")\n",
    "markdown_content = \"\"\"\n",
    "---\n",
    "title: FAQ\n",
    "sidebar_position: 3\n",
    "image: og/docs/more-resources.jpg\n",
    "# tags: ['FAQ']\n",
    "---\n",
    "\n",
    "\n",
    "## General\n",
    "\n",
    "#### Q: Why would I use Weaviate as my vector database?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Our goal is three-folded. Firstly, we want to make it as easy as possible for others to create their own semantic systems or vector search engines (hence, our APIs are GraphQL based). Secondly, we have a strong focus on the semantic element (the \"knowledge\" in \"vector databases,\" if you will). Our ultimate goal is to have Weaviate help you manage, index, and \"understand\" your data so that you can build newer, better, and faster applications. And thirdly, we want you to be able to run it everywhere. This is the reason why Weaviate comes containerized.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: What is the difference between Weaviate and for example Elasticsearch?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Other database systems like Elasticsearch rely on inverted indices, which makes search super fast. Weaviate also uses inverted indices to store data and values. But additionally, Weaviate is also a vector-native search database, which means that data is stored as vectors, which enables semantic search. This combination of data storage is unique, and enables fast, filtered and semantic search from end-to-end.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Do you offer Weaviate as a managed service?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Yes, we do - check out [Weaviate Cloud Services](/pricing).\n",
    "\n",
    "</details>\n",
    "\n",
    "## Configuration and setup\n",
    "\n",
    "#### Q: How should I configure the size of my instance?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> You can find this in the [architecture section](/developers/weaviate/concepts/resources.md#an-example-calculation) of the docs.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Do I need to know about Docker (Compose) to use Weaviate?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Weaviate uses Docker images as a means to distribute releases and uses Docker Compose to tie a module-rich runtime together. If you are new to those technologies, we recommend reading the [Docker Introduction for Weaviate Users](https://medium.com/semi-technologies/what-weaviate-users-should-know-about-docker-containers-1601c6afa079).\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: What happens when the Weaviate Docker container restarts? Is my data in the Weaviate database lost?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> There are three levels:\n",
    "> 1. You have no volume configured (the default in our `Docker Compose` files), if the container restarts (e.g. due to a crash, or because of `docker stop/start`) your data is kept\n",
    "> 2. You have no volume configured (the default in our `Docker Compose` files), if the container is removed (e.g. from `docker compose down` or `docker rm`) your data is gone\n",
    "> 3. If a volume is configured, your data is persisted regardless of what happens to the container. They can be completely removed or replaced, next time they start up with a volume, all your data will be there\n",
    "\n",
    "</details>\n",
    "\n",
    "## Schema and data structure\n",
    "\n",
    "#### Q: Are there any 'best practices' or guidelines to consider when designing a schema?\n",
    "\n",
    "*(E.g. if I was looking to perform a semantic search over a the content of a Book would I look to have Chapter and Paragraph represented in the schema etc, would this be preferred over including the entire content of the novel in a single property?)*\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> As a rule of thumb, the smaller the units, the more accurate the search will be. Two objects of e.g. a sentence would most likely contain more information in their vector embedding than a common vector (which is essentially just the mean of sentences). At the same time more objects leads to a higher import time and (since each vector also makes up some data) more space. (E.g. when using transformers, a single vector is 768xfloat32 = 3KB. This can easily make a difference if you have millions, etc.) of vectors. As a rule of thumb, the more vectors you have the more memory you're going to need.\n",
    ">\n",
    "> So, basically, it's a set of tradeoffs. Personally we've had great success with using paragraphs as individual units, as there's little benefit in going even more granular, but it's still much more precise than whole chapters, etc.\n",
    ">\n",
    "> You can use cross-references to link e.g. chapters to paragraphs. Note that resolving a cross-references takes a slight performance penalty. Essentially resolving A1->B1 is the same cost as looking up both A1 and B1 indvidually. This cost however, will probably only matter at really large scale.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Should I use references in my schema?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> In short: for convenience you can add relations to your data schema, because you need less code and queries to get data. But resolving references in queries takes some of the performance.\n",
    ">\n",
    "> 1. If your ultimate goal is performance, references probably don't add any value, as resolving them adds a cost.\n",
    "> 2. If your goal is represent complex relationships between your data items, they can help a lot. You can resolve references in a single query, so if you have classes with multiple links, it could definitely be helpful to resolve some of those connections in a single query. On the other hand, if you have a single (bi-directional) reference in your data, you could also just denormalize the links (e.g. with an ID field) and resolve them during search.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Is it possible to create one-to-many relationships in the schema?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Yes, it is possible to reference to one or more objects (Class -> one or more Classes) through cross-references. Referring to lists or arrays of primitives, this will be available [soon](https://github.com/weaviate/weaviate/issues/1611).\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: What is the difference between `text` and `string` and `valueText` and `valueString`?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> The `text` and `string` datatypes differ in tokenization behavior. Note that `string` is now deprecated. Read more in [this section](../config-refs/schema/index.md#property-tokenization) on the differences.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Do Weaviate classes have namespaces?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "Yes. Each class itself acts like namespaces. Additionally, you can use the [multi-tenancy](../concepts/data.md#multi-tenancy) feature to create isolated storage for each tenant. This is especially useful for use cases where one cluster might be used to store data for multiple customers or users.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Are there restrictions on UUID formatting? Do I have to adhere to any standards?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> The UUID must be presented as a string matching the [Canonical Textual representation](https://en.wikipedia.org/wiki/Universally_unique_identifier#Format). If you don't specify a UUID, Weaviate will generate a `v4` i.e. a random UUID. If you generate them yourself you could either use random ones or deterministically determine them based on some fields that you have. For this you'll need to use [`v3` or `v5`](https://en.wikipedia.org/wiki/Universally_unique_identifier#Versions_3_and_5_(namespace_name-based)).\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: If I do not specify a UUID during adding data objects, will Weaviate create one automatically?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Yes, a UUID will be created if not specified.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Can I use Weaviate to create a traditional knowledge graph?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Yes, you can! Weaviate support ontology, RDF-like definitions in its schema, and it runs out of the box. It is scalable, and the GraphQL API will allow you to query through your knowledge graph easily. But now you are here. We like to suggest you really try its semantic features. After all, you are creating a _knowledge_ graph ðŸ˜‰.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Why does Weaviate have a schema and not an ontology?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> We use a schema because it focusses on the representation of your data (in our case in the GraphQL API) but you can use a Weaviate schema to express an ontology. One of Weaviate's core features is that it semantically interprets your schema (and with that your ontology) so that you can search for concepts rather than formally defined entities.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: What is the difference between a Weaviate data schema, ontologies and taxonomies?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Read about how taxonomies, ontologies and schemas are related to Weaviate in [this blog post](https://medium.com/semi-technologies/taxonomies-ontologies-and-schemas-how-do-they-relate-to-weaviate-9f76739fc695).\n",
    "\n",
    "</details>\n",
    "\n",
    "## Text and language processing\n",
    "\n",
    "#### Q: How to deal with custom terminology?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Sometimes, users work with custom terminology, which often comes in the form of abbreviations or jargon. You can find more information on how to use the endpoint [here](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-contextionary.md#extending-the-contextionary-v1modulestext2vec-contextionaryextensions)\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: How can you index data near-realtime without losing semantic meaning?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Every data object [gets its vector representation](../) based on its semantic meaning. In a nutshell, we calculate the vector position of the data object based on the words and concepts used in the data object. The existing model in the contextionary gives already enough context. If you want to get in the nitty-gritty, you can [browse the code here](https://github.com/weaviate/contextionary/tree/master/server), but you can also ask a [specific question on Stackoverflow](https://stackoverflow.com/tags/weaviate/) and tag it with Weaviate.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Why isn't there a text2vec-contextionary in my language?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Because you are probably one of the first that needs one! Ping us [here on GitHub](https://github.com/weaviate/weaviate/issues), and we will make sure in the next iteration it will become available (unless you want it in [Silbo Gomero](https://en.wikipedia.org/wiki/Silbo_Gomero) or another language which is whistled).\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: How do you deal with words that have multiple meanings?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> How can Weaviate interpret that you mean a company, as in business, and not as the division of the army? We do this based on the structure of the schema and the data you add. A schema in Weaviate might contain a company class with the property name and the value Apple. This simple representation (company, name, apple) is already enough to gravitate the vector position of the data object towards businesses or the iPhone. You can read [here](../) how we do this, or you can ask a specific question on [Stackoverflow](https://stackoverflow.com/tags/weaviate/) and tag it with Weaviate.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Is there support to multiple versions of the query/document embedding models to co-exist at a given time? (helps with live experiments of new model versions)\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> You can create multiple classes in the Weaviate schema, where one class will act like a namespace in Kubernetes or an index in Elasticsearch. So the spaces will be completely independent, this allows space 1 to use completely different embeddings from space 2. The configured vectorizer is always scoped only to a single class. You can also use Weaviate's Cross-Reference features to make a graph-like connection between an object of Class 1 to the corresponding object of Class 2 to make it easy to see the equivalent in the other space.\n",
    "\n",
    "</details>\n",
    "\n",
    "## Queries\n",
    "\n",
    "#### Q: How can I retrieve the total object count in a class?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "import HowToGetObjectCount from '/_includes/how.to.get.object.count.mdx';\n",
    "\n",
    "> This `Aggregate` query returns the total object count in a class.\n",
    "\n",
    "<HowToGetObjectCount/>\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: How do I get the cosine similarity from Weaviate's certainty?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> To obtain the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) from weaviate's `certainty`, you can do `cosine_sim = 2*certainty - 1`\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: The quality of my search results change depending on the specified limit. Why? How can I fix this?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "Weaviate makes use of ANN indices to serve vector searches. An ANN index is an approximate nearest neighbor index. The \"approximate\" part refers to an explicit recall-query-speed tradeoff. This trade-off is presented in detail in the [ANN benchmarks section](/developers/weaviate/benchmarks/ann.md#results). For example, a 98% recall for a given set of HNSW parameters means that 2% of results will not match the true nearest neighbors. What build parameters lead to what recall depends on the dataset used. The benchmark pages shows 4 different example datasets. Based on the characteristic of each dataset you can pick the one closest to your production load and draw conclusions about the expected recall for the respective build and query-time parameters.\n",
    "\n",
    "Generally if you need a higher recall than the default parameters provide you with, you can use stronger parameters. This can either be done at build time (`efConstruction`, `maxConnections`) or at query time (`ef`). Roughly speaking, a higher `ef` value at query time means a more thorough search. It will have a slightly higher latency, but also lead to a slightly better recall.\n",
    "\n",
    "By changing the specified limit, you are implicitly changing the `ef` parameter. This is because the default `ef` value is set to `-1`, indicating that Weaviate should pick the parameter based on the limit. The dynamic `ef` value is controlled using the configuration fields `dynamicEfMin` which acts as a lower boundary, `dynamicEfMax` which acts as an upper boundary and `dynamicEfFactor` which is the factor to derive the target `ef` based on the limit within the lower and upper boundary.\n",
    "\n",
    "Example: Using the default parameters `ef=-1`, `dynamicEfMin=100`, `dynamicEfMax=500`, `dynamicEfFactor=8`, you will end up with the following `ef` values based on the limit:\n",
    "\n",
    "* `limit=1`, dynamically calculated: `ef=1*8=8`. This value is below the lower boundary, so `ef` is set to `100`.\n",
    "* `limit=20`, dynamically calculated: `ef=20*8=160`. This value is within the boundaries, so `ef` is `160`.\n",
    "* `limit=100`, dynamically calculated: `ef=100*8=800`. This value is above the upper boundary, so `ef` is set to `500`.\n",
    "\n",
    "If you need a higher search quality for a given limit you can consider the following options:\n",
    "\n",
    "1. Instead of using a dynamic `ef` value, use a fixed one that provides the desired recall.\n",
    "1. If your search quality varies a lot depending on the query-time `ef` values, you should also consider choosing stronger build parameters. The [ANN benchmarks section](/developers/weaviate/benchmarks/ann.md#results) present a combination of many different parameter combination for various datasets.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Why did you use GraphQL instead of SPARQL?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> For user experience. We want to make it as simple as possible to integrate Weaviate into your stack, and we believe that GraphQL is the answer to this. The community and client libraries around GraphQL are enormous, and you can use almost all of them with Weaviate.\n",
    "\n",
    "</details>\n",
    "\n",
    "## Data management\n",
    "\n",
    "#### Q: What is the best way to iterate through objects? Can I do paginated API calls?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Yes, Weaviate supports cursor-based iteration as well as pagination through a result set.\n",
    ">\n",
    "> To iterate through all objects, you can use the `after` operator with both [REST](../api/rest/objects.md#exhaustive-listing-using-a-cursor-after) and [GraphQL](../api/graphql/additional-operators.md#cursor-with-after).\n",
    ">\n",
    "> For pagination through a result set, you can use the `offset` and `limit` operators for GraphQL API calls. Take a look at [this page](../api/graphql/filters.md#pagination-with-offset) which describes how to use these operators, including tips on performance and limitations.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: What is best practice for updating data?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Here are top 3 best practices for updating data:\n",
    "> 1. Use the [batch API](../api/rest/batch.md)\n",
    "> 2. Start with a small-ish batch size e.g. 100 per batch. Adjust up if it is very fast, adjust down if you run into timeouts\n",
    "> 3. If you have unidirectional relationships (e.g. `Foo -> Bar`.) it's easiest to first import all `Bar` objects, then import all `Foo` objects with the refs already set. If you have more complex relationships, you can also import the objects without references, then use the [`/v1/batch/references API`](../api/rest/batch.md) to set links between classes in arbitrary directions.\n",
    "\n",
    "</details>\n",
    "\n",
    "## Modules\n",
    "\n",
    "#### Q: Can I connect my own module?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> [Yes!](/developers/weaviate/modules/other-modules/custom-modules.md)\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Can I train my own text2vec-contextionary vectorizer module?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Not at the moment. You can currently use the [available contextionaries](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-contextionary.md) in a variety of languages and use the transfer learning feature to add custom concepts if needed.\n",
    "\n",
    "</details>\n",
    "\n",
    "## Indexes in Weaviate\n",
    "\n",
    "#### Q: Does Weaviate use Hnswlib?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> No\n",
    ">\n",
    "> Weaviate uses a custom implementation of HNSW that overcomes certain limitations of [hnswlib](https://github.com/nmslib/hnswlib), such as durability requirements, CRUD support, pre-filtering, etc.\n",
    ">\n",
    "> Custom HNSW implementation in Weaviate references:\n",
    ">\n",
    "> - [HNSW plugin (GitHub)](https://github.com/weaviate/weaviate/tree/master/adapters/repos/db/vector/hnsw)\n",
    "> - [vector dot product ASM](https://github.com/weaviate/weaviate/blob/master/adapters/repos/db/vector/hnsw/distancer/asm/dot_amd64.s)\n",
    ">\n",
    "> More information:\n",
    ">\n",
    "> - [Weaviate, an ANN Database with CRUD support â€“ DB-Engines.com](https://db-engines.com/en/blog_post/87) â¬…ï¸ best resource on the topic\n",
    "> - [Weaviate's HNSW implementation in the docs](/developers/weaviate/concepts/vector-index.md#hnsw)\n",
    ">\n",
    "> _Note I: HNSW is just one implementation in Weaviate, but Weaviate can support multiple indexing algoritmns as outlined [here](/developers/weaviate/concepts/vector-index.md)_\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Are all ANN algorithms potential candidates to become an indexation plugin in Weaviate?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> No\n",
    ">\n",
    "> Some algorithms (e.g., Annoy or ScaNN) are entirely immutable once built, they can neither be changed nor built up incrementally. Instead, they require you to have all of your vectors present, then you build the algorithm once. After a build, you can only query them, but cannot add more elements or change existing elements. Thus, they aren't capable of the CRUD operations we want to support in Weaviate.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Does Weaviate use pre- or post-filtering ANN index search?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Weaviate currently uses pre-filtering exclusively on filtered ANN search.\n",
    "> See \"How does Weaviate's vector and scalar filtering work\" for more details.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: How does Weaviate's vector and scalar filtering work?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> It's a 2-step process:\n",
    ">\n",
    "> 1. The inverted index (which is [built at import time](#q-does-weaviate-use-hnswlib)) queries to produce an allowed list of the specified document ids. Then the ANN index is queried with this allow list (the list being one of the reasons for our custom implementation).\n",
    "> 2. If we encounter a document id which would be a close match, but isn't on the allow list the id is treated as a candidate (i.e. we add it to our list of links to evaluate), but is never added to the result set. Since we only add allowed IDs to the set, we don't exit early, i.e. before the top `k` elements are reached.\n",
    ">\n",
    "> For more information on the technical implementations, see [this video](https://www.youtube.com/watch?v=6hdEJdHWXRE).\n",
    "\n",
    "</details>\n",
    "\n",
    "#### What is the maximum number of vector dimensions for embeddings?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> As the embedding is currently stored using `uint16`, the maximum possible length is currently 65535.\n",
    "\n",
    "</details>\n",
    "\n",
    "## Performance\n",
    "\n",
    "#### Q: What would you say is more important for query speed in Weaviate: More CPU power, or more RAM?\n",
    "\n",
    "More concretely: If you had to pick between a machine that has 16 GB of RAM and 2 CPUs, or a machine that has 8 GB of RAM and 4 CPUs, which would you pick?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> This is a very difficult to answer 100% correctly, because there are several factors in play:\n",
    "> * **The vector search itself**. This part is CPU-bound, however only with regards to throughput: A single search is single-threaded. Multiple parallel searches can use multiple threads. So if you measure the time of a single request (otherwise idle), it will be the same whether the machine has 1 core or 100. However, if your QPS approach the throughput of a CPU, you'll see massive benefits by adding more Cores\n",
    "> * **The retrieval of the objects**. Once the vector search part is done, we are essentially left with a list of n IDs which need to be resolved to actual objects. This is IO-bound in general. However, all disk files are memory-mapped. So generally, more mem will allow you to hold more of the disk state in memory. In real life however, it's not that simple. Searches are rarely evenly distributed. So let's pretend that 90% of searches will return just 10% of objects (because these are more popular search results). Then if those 10% of the disk objects are already cached in mem, there's no benefit in adding more memory.\n",
    ">\n",
    "> Taking the above in mind: we can carefully say: If throughput is the problem, increase CPU, if response time is the problem increase mem. However, note that the latter only adds value if there are more things that can be cached. If you have enough mem to cache your entire disk state (or at least the parts that are relevant for most queries), additional memory won't add any additional benefit.\n",
    "> If we are talking about imports on the other hand, they are almost always CPU-bound because of the cost of creating the HNSW index. So, if you can resize between import and query, my recommendation would be roughly prefer CPUs while importing and then gradually replace CPU with memory at query time - until you see no more benefits. (This assumes that there is a separation between importing and querying which might not always be the case in real life).\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Data import takes long / is slow, what is causing this and what can I do?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> HNSW is super fast at query time, but slower on vectorization. This means that adding and updating data objects costs relatively more time. You could try [asynchronous indexing](../config-refs/schema/vector-index.md#asynchronous-indexing), which separates data ingestion from vectorization.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: How can slow queries be optimized?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Queries containing deeply nested references that need to be filtered or resolved can take some time. Read on optimization strategies [here](./performance.md#costs-of-queries-and-operations).\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "#### Q: When scalar and vector search are combined, will the scalar filter happen before or after the nearest neighbor (vector) search?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> The mixed structured vector searches in Weaviate are pre-filter. There is an inverted index which is queried first to basically form an allow-list, in the HNSW search the allow list is then used to treat non-allowed doc ids only as nodes to follow connections, but not to add to the result set.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: Regarding \"filtered vector search\": Since this is a two-phase pipeline, how big can that list of IDs get? Do you know how that size might affect query performance?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Essentially the list ids uses the internal doc id which is a `uint64` or 8 bytes per ID. The list can grow as long as you have memory available. So for example with 2GB of free memory, it could hold 250M ids, with 20GB it could hold 2.5B ids, etc.\n",
    ">\n",
    "> Performance wise there are two things to consider:\n",
    "> 1. Building the lookup list\n",
    "> 2. Filtering the results when vector searching\n",
    ">\n",
    "> Building the list is a typical inverted index look up, so depending on the operator this is just a single read on == (or a set of range reads, e.g. for >7, we'd read the value rows from 7 to infinity). This process is pretty efficient, similar to how the same thing would happen in a traditional search engine, such as elasticsearch\n",
    ">\n",
    "> Performing the filtering during the vector search depends on whether the filter is very restrictive or very loose. In the case you mentioned where a lot of IDs are included, it will be very efficient. Because the equivalent of an unfiltered search would be the one where your ID list contains all possible IDs. So the HNSW index would behave normally. There is however, a small penalty whenever a list is present: We need to check if the current ID is contained an the allow-list. This is essentially a hashmap lookup, so it should be O(1) per object. Nevertheless, there is a slight performance penalty.\n",
    ">\n",
    "> Now the other extreme, a very restrictive list, i.e few IDs on the list, actually takes considerably more time. Because the HNSW index will find neighboring IDs, but since they're not contained, they cannot be added as result candidates, meaning that all we can do with them is evaluating their connections, but not the points themselves. In the extreme case of a list that is very, very restrictive, say just 10 objects out of 1B in the worst case the search would become exhaustive if you the filtered ids are very far from the query. In this extreme case, it would actually be much more efficient to just skip the index and do a brute-force indexless vector search on the 10 ids. So, there is a cut-off when a brute-force search becomes more efficient than a heavily-restricted vector search with HNSW. We do not yet have any optimization to discovery such a cut-off point and skip the index, but this should be fairly simple to implement if this ever becomes an actual problem.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: My Weaviate setup is using more memory than what I think is reasonable. How can I debug this?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> First of all, make sure your import runs with the latest version of Weaviate, since `v1.12.0`/`v1.12.1` fixed an issue where [too much data was written to disk](https://github.com/weaviate/weaviate/issues/1868) which then lead to unreasonable memory consumption after restarts. If this did not fix the issue yet, please see this post on [how to profile the memory usage of a Weaviate setup](https://stackoverflow.com/a/71793178/5322199).\n",
    "\n",
    "</details>\n",
    "\n",
    "## Troubleshooting / debugging\n",
    "\n",
    "#### Q: How can I print a stack trace of Weaviate?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "You can do this by sending a `SIGQUIT` signal to the process. This will print a stack trace to the console. The logging level and debugging variables can be set with `LOG_LEVEL` and `DEBUG` [environment variables](https://weaviate.io/developers/weaviate/config-refs/env-vars).\n",
    "\n",
    "Read more on SIGQUIT [here](https://en.wikipedia.org/wiki/Signal_(IPC)#SIGQUIT) and this [StackOverflow answer](https://stackoverflow.com/questions/19094099/how-to-dump-goroutine-stacktraces/35290196#35290196).\n",
    "\n",
    "</details>\n",
    "\n",
    "## Miscellaneous\n",
    "\n",
    "#### Q: Can I request a feature in Weaviate?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Sure (also, feel free to [issue a pull request](https://github.com/weaviate/weaviate/pulls) ðŸ˜‰) you can [add those requests here](https://github.com/weaviate/weaviate/issues). The only thing you need is a GitHub account, and while you're there, make sure to give us a star ðŸ˜‡.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: What is Weaviate's consistency model in a distributed setup?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> Weaviate is generally modeled to prefer Availability over Consistency (AP over CP). It is designed to deliver low search latencies under high throughput in situations where availability is more business-critical than consistency. If strict serializability is required on your data, we generally recommend storing your data in a different primary data store, use Weaviate as an auxiliary data store, and set up replication between the two. If you do not need serializability and eventual consistency is enough for your use case, Weaviate can be used as a primary datastore.\n",
    ">\n",
    "> Weaviate has no notion of transactions, operations always affect exactly a single key, therefore Serializability is not applicable. In a distributed setup (under development) Weaviate's consistency model is eventual consistency. When a cluster is healthy, all changes are replicated to all affected nodes by the time the write is acknowledged by the user. Objects will immediately be present in search results on all nodes after the import request completes. If a search query occurs concurrently with an import operation nodes may not be in sync yet. This means some nodes might already include the newly added or updated objects, while others don't yet. In a healthy cluster, all nodes will have converged by the time the import request has been completed successfully. If a node is temporarily unavailable and rejoins a cluster it may temporarily be out of sync. It will then sync the missed changes from other replica nodes and eventually serve the same data again.\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: With your aggregations I could not see how to do time buckets, is this possible?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> At the moment, we cannot aggregate over timeseries into time buckets yet, but architecturally there's nothing in the way. If there is demand, this seems like a nice feature request, you can submit an [issue here](https://github.com/weaviate/weaviate/issues). (We're a very small company though and the priority is on Horizontal Scaling at the moment.)\n",
    "\n",
    "</details>\n",
    "\n",
    "#### Q: How can I run the latest master branch with Docker Compose?\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "\n",
    "> You can run Weaviate with `Docker Compose`, you can build your own container off the [`master`](https://github.com/weaviate/weaviate) branch. Note that this is not an officially released Weaviate version, so this might contain bugs.\n",
    ">\n",
    "> ```sh\n",
    "> git clone https://github.com/weaviate/weaviate.git\n",
    "> cd weaviate\n",
    "> docker build --target weaviate -t name-of-your-weaviate-image .\n",
    "> ```\n",
    ">\n",
    "> Then, make a `docker-compose.yml` file with this new image. For example:\n",
    ">\n",
    "> ```yml\n",
    "> version: '3.4'\n",
    "> services:\n",
    ">   weaviate:\n",
    ">     image: name-of-your-weaviate-image\n",
    ">     ports:\n",
    ">       - 8080:8080\n",
    ">     environment:\n",
    ">       CONTEXTIONARY_URL: contextionary:9999\n",
    ">       QUERY_DEFAULTS_LIMIT: 25\n",
    ">       AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n",
    ">       PERSISTENCE_DATA_PATH: './data'\n",
    ">       ENABLE_MODULES: 'text2vec-contextionary'\n",
    ">       DEFAULT_VECTORIZER_MODULE: 'text2vec-contextionary'\n",
    ">       AUTOSCHEMA_ENABLED: 'false'\n",
    ">   contextionary:\n",
    ">     environment:\n",
    ">       OCCURRENCE_WEIGHT_LINEAR_FACTOR: 0.75\n",
    ">       EXTENSIONS_STORAGE_MODE: weaviate\n",
    ">       EXTENSIONS_STORAGE_ORIGIN: http://weaviate:8080\n",
    ">       NEIGHBOR_OCCURRENCE_IGNORE_PERCENTILE: 5\n",
    ">       ENABLE_COMPOUND_SPLITTING: 'false'\n",
    ">     image: semitechnologies/contextionary:en0.16.0-v1.0.2\n",
    "> ```\n",
    ">\n",
    "> After the build is complete, you can run this Weaviate build with docker compose:\n",
    "\n",
    "```bash\n",
    "docker compose up\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "## More questions?\n",
    "\n",
    "import DocsMoreResources from '/_includes/more-resources-docs.md';\n",
    "\n",
    "<DocsMoreResources />\n",
    "\"\"\"\n",
    "\n",
    "def parse_questions(markdown_content):\n",
    "    # Regular expression pattern for finding questions\n",
    "    question_pattern = r'#### Q: (.+?)\\n'\n",
    "\n",
    "    # Finding all questions\n",
    "    questions = re.findall(question_pattern, markdown_content, re.DOTALL)\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Parsing the markdown content to get only questions\n",
    "questions = parse_questions(markdown_content)\n",
    "\n",
    "# Displaying the first few extracted questions\n",
    "questions[:5]  # Displaying only the first few for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89745ebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T03:03:22.327036Z",
     "start_time": "2024-03-18T03:03:22.309961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c34f8d",
   "metadata": {},
   "source": [
    "# Wrap each FAQ into an `Example` object\n",
    "\n",
    "The dspy `Example` object optionally lets you attach metadata, or additional labels, to input/output pairs.\n",
    "\n",
    "For example, you may want to jointly supervise the answer as well as the context the retrieval system produced to feed into the answer generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449b13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into dspy datasets\n",
    "import dspy\n",
    "\n",
    "# ToDo, add random splitting -- maybe wrap this entire thing in a cross-validation loop\n",
    "trainset = questions[:20] # 20 examples for training\n",
    "devset = questions[20:30] # 10 examples for development\n",
    "testset = questions[30:] # 14 examples for testing\n",
    "\n",
    "trainset = [dspy.Example(question=question).with_inputs(\"question\") for question in trainset]\n",
    "devset = [dspy.Example(question=question).with_inputs(\"question\") for question in devset]\n",
    "testset = [dspy.Example(question=question).with_inputs(\"question\") for question in testset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a884b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175ab89",
   "metadata": {},
   "source": [
    "# 2. LLM Metrics\n",
    "\n",
    "Define a Metric for Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a5411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a WIP, the next step is to optimize this metric as itself a DSPy module (pretty meta)\n",
    "\n",
    "# Reference - https://github.com/stanfordnlp/dspy/blob/main/examples/tweets/tweet_metric.py\n",
    "\n",
    "metricLM = dspy.OpenAI(model='gpt-4', max_tokens=1000, model_type='chat')\n",
    "\n",
    "# Signature for LLM assessments.\n",
    "\n",
    "class Assess(dspy.Signature):\n",
    "    \"\"\"Assess the quality of an answer to a question.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"The context for answering the question.\")\n",
    "    assessed_question = dspy.InputField(desc=\"The evaluation criterion.\")\n",
    "    assessed_answer = dspy.InputField(desc=\"The answer to the question.\")\n",
    "    assessment_answer = dspy.OutputField(desc=\"A rating between 1 and 5. Only output the rating and nothing else.\")\n",
    "\n",
    "def llm_metric(gold, pred, trace=None):\n",
    "    predicted_answer = pred.answer\n",
    "    question = gold.question\n",
    "    \n",
    "    print(f\"Test Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    \n",
    "    detail = \"Is the assessed answer detailed?\"\n",
    "    faithful = \"Is the assessed text grounded in the context? Say no if it includes significant facts not in the context.\"\n",
    "    overall = f\"Please rate how well this answer answers the question, `{question}` based on the context.\\n `{predicted_answer}`\"\n",
    "    \n",
    "    with dspy.context(lm=metricLM):\n",
    "        context = dspy.Retrieve(k=5)(question).passages\n",
    "        detail = dspy.ChainOfThought(Assess)(context=\"N/A\", assessed_question=detail, assessed_answer=predicted_answer)\n",
    "        faithful = dspy.ChainOfThought(Assess)(context=context, assessed_question=faithful, assessed_answer=predicted_answer)\n",
    "        overall = dspy.ChainOfThought(Assess)(context=context, assessed_question=overall, assessed_answer=predicted_answer)\n",
    "    \n",
    "    print(f\"Faithful: {faithful.assessment_answer}\")\n",
    "    print(f\"Detail: {detail.assessment_answer}\")\n",
    "    print(f\"Overall: {overall.assessment_answer}\")\n",
    "    \n",
    "    \n",
    "    total = float(detail.assessment_answer) + float(faithful.assessment_answer)*2 + float(overall.assessment_answer)\n",
    "    \n",
    "    return total / 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cc41a",
   "metadata": {},
   "source": [
    "## Inspect the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = dspy.Example(question=\"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer=\"They re-rank documents.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f763ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = dspy.Example(question=\"What do cross encoders do?\")\n",
    "test_pred = dspy.Example(answer=\"They index data.\")\n",
    "\n",
    "type(llm_metric(test_example, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ccd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5202b5",
   "metadata": {},
   "source": [
    "# 3. The DSPy Programming Model\n",
    "\n",
    "This block of first code will initilaize the `GenerateAnswer` signature.\n",
    "\n",
    "Then we will compose a `dspy.Module` consisting of:\n",
    "- Retrieve\n",
    "- GenerateAnswer\n",
    "\n",
    "The DSPy programming model is one of the most powerful aspects of DSPy, we get:\n",
    "- An intuitive interface to compose prompts into programs.\n",
    "- A clean way to organize prompts into Signatures.\n",
    "- Structured output parsing with `dspy.OutputField`\n",
    "- Built-in prompt extensions such as `ChainOfThought`, `ReAct`, and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5ce19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer questions based on the context.\"\"\"\n",
    "    \n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self, num_passages=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = self.retrieve(question).passages\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743dda11",
   "metadata": {},
   "source": [
    "# A little more info on built-in dspy modules\n",
    "\n",
    "The DSPy programming model gives you a lot of cool features out of the box. Observe how different modules implement signatures with additional prompting techniques like `ChainOfThought` and `ReAct`. `Predict` is the base class to observe what a standrd prompt looks like without the module extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789b0cd",
   "metadata": {},
   "source": [
    "### dspy.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.Predict(GenerateAnswer)(question=\"What are Cross Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2cfb6",
   "metadata": {},
   "source": [
    "### dspy.ChainOfThought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e194e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.ChainOfThought(GenerateAnswer)(question=\"What are Cross Encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5666dd3",
   "metadata": {},
   "source": [
    "### dspy.ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.ReAct(GenerateAnswer, tools=[dspy.settings.rm])(question=\"What are cross encoders?\")\n",
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987c317",
   "metadata": {},
   "source": [
    "# Initialize DSPy Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8abafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompiled_rag = RAG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5523912",
   "metadata": {},
   "source": [
    "# Test uncompiled inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efc6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncompiled_rag(\"What are re-rankers in search engines?\").answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca3b3c",
   "metadata": {},
   "source": [
    "# Check the last call to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa7a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3dda6",
   "metadata": {},
   "source": [
    "# 4. DSPy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ce3c72",
   "metadata": {},
   "source": [
    "# Evaluate our RAG Program before it is compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfccd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder our dataset looks like this:\n",
    "\n",
    "devset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc24f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "evaluate = Evaluate(devset=devset, num_threads=1, display_progress=True, display_table=5)\n",
    "\n",
    "evaluate(RAG(), metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ee450",
   "metadata": {},
   "source": [
    "# Metric Analysis\n",
    "\n",
    "The maximum value per rating is (5 + 5*2 + 5) / 5 = 4\n",
    "\n",
    "4 * 10 test questions = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metricLM.inspect_history(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9978562c",
   "metadata": {},
   "source": [
    "# BootstrapFewShot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18712073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "teleprompter = BootstrapFewShot(metric=llm_metric, max_labeled_demos=8, max_rounds=3)\n",
    "\n",
    "# also common to init here, e.g. Rag()\n",
    "compiled_rag = teleprompter.compile(uncompiled_rag, trainset=trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd62e57",
   "metadata": {},
   "source": [
    "### Inspect the compiled prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_rag(\"What do cross encoders do?\").answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed105e",
   "metadata": {},
   "source": [
    "### Evaluate the Compiled RAG Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8926c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789072a8",
   "metadata": {},
   "source": [
    "# BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accidentally spent $12 on this with `num_candidate_programs=20`, caution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=llm_metric, \n",
    "                                                max_bootstrapped_demos=4,\n",
    "                                                max_labeled_demos=4, \n",
    "                                                max_rounds=1,\n",
    "                                                num_candidate_programs=2,\n",
    "                                                num_threads=2)\n",
    "\n",
    "# also common to init here, e.g. Rag()\n",
    "second_compiled_rag = teleprompter.compile(uncompiled_rag, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6302e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_compiled_rag(\"What do cross encoders do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(second_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f1e2b1",
   "metadata": {},
   "source": [
    "# BayesianSignatureOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e272916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BayesianSignatureOptimizer\n",
    "\n",
    "llm_prompter = dspy.OpenAI(model='gpt-4', max_tokens=2000, model_type='chat')\n",
    "\n",
    "teleprompter = BayesianSignatureOptimizer(task_model=dspy.settings.lm,\n",
    "                                          metric=llm_metric,\n",
    "                                          prompt_model=llm_prompter,\n",
    "                                          n=5,\n",
    "                                          verbose=False)\n",
    "\n",
    "kwargs = dict(num_threads=1, display_progress=True, display_table=0)\n",
    "third_compiled_rag = teleprompter.compile(RAG(), devset=devset,\n",
    "                                         optuna_trials_num=3,\n",
    "                                         max_bootstrapped_demos=4,\n",
    "                                         max_labeled_demos=4,\n",
    "                                         eval_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_compiled_rag(\"What do cross encoders do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367bcc7",
   "metadata": {},
   "source": [
    "# Check this out!!\n",
    "\n",
    "Below you can see how the BayesianSignatureOptimizer jointly (1) optimizes the task instruction to:\n",
    "\n",
    "```\n",
    "Assess the context and answer the given questions that are predominantly about software usage, process optimization, and troubleshooting. Focus on providing accurate information related to tech or software-related queries.\n",
    "```\n",
    "\n",
    "As well as sourcing input-output examples for the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98db525",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(third_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecdc368",
   "metadata": {},
   "source": [
    "# Test Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Uncompiled\n",
    "from dspy.evaluate.evaluate import Evaluate\n",
    "\n",
    "# Set up the `evaluate_on_hotpotqa` function. We'll use this many times below.\n",
    "evaluate = Evaluate(devset=testset, num_threads=1, display_progress=True, display_table=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec6960",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(uncompiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b00bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(second_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0790a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(third_compiled_rag, metric=llm_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76b0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
